---
title: "PrimaryForest_Modelling"
author: "Francesco Maria Sabatini"
date: "25 8 2021"
output: html_document
---

Primary forests have high conservation value but are rare in Europe due to historic land use. Yet many primary forest patches remain unmapped, especially in the Carpathians, which are the stronghold of primary forests in Europe. 
Based on data contained in the [European Primary Forest Database v2.0](https://figshare.com/articles/dataset/European_Primary_Forest_Database/13194095) [(Sabatini et al. 2021 - DDI)](https://www.nature.com/articles/s41597-021-00988-7), we will train a model to predict where still-to-be-mapped primary forests are likely to be located in the Carpathians. 
As predictors, we will use a selection of variables related to topography, climate and land-use.
Our model will be generated using an approach derived from Species Distribution Modelling (SDM), which relies on the use of a Machine-Learning algorithm - Boosted Regression Trees (BRTs). 
This is a powerful algorithm which requires little or no assumptions, and can flexibly model non-linear relationships and multi-level interactions. 
BRTs rely on regression trees, which partition the predictor space into rectangles, using a series of rules to identify regions having the most homogeneous responses to predictors, and then fit a constant to each region. 
The main difference is that BRTs do not produce a single ‘best’ model. 
Rather, they use the technique of boosting to combine large numbers of relatively simple tree models adaptively, to minimize the unexplained variation by adding, at each step, a new tree which maximally reduces the residuals.

<br>
For a guide to BRTs see [Elith et al. (2008)](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.1365-2656.2008.01390.x) and the [vignette of the Dismo package](https://rspatial.org/raster/sdm/). 



<br> 
Goals of this exercise:  
1. Get familiar with the manipulation of spatial data in R  
2. Create a model predicting the likelihood of occurrence of primary forests stands in the Carpathians using BRTs  
3. Validate the model and explore the output  
4. Discuss the output & the limitations of the approach  

<br>


## Install and load packages
Make sure you have all the packages installed, first. 
```{r, message=F, warning=F}
# install.packages(c("tidyverse", "raster", "st", "rgdal", "gbm", "dismo", "blockCV", "automap")) if necessary
library(tidyverse)
library(raster)
library(sf)
library(rgdal)
library(gbm)
library(dismo)
library(blockCV)
library(automap)
```

## Prepare geographical data
Import and visualize spatial predictors. Convert to WGS84
```{r, message=F, warning=F, fig.align="center", fig.height=8, fig.width=10, cache=T}
files <- list.files(path="../_data", pattern="*.tif$", full.names = T)
mystack <- raster::stack(files)
mystack <- projectRaster(mystack, crs=crs("+proj=longlat +datum=WGS84 +no_defs"), method = "ngb")
plot(mystack)
```

A description of the predictors, with the related sources, is in Table 1 from [Sabatini et al. 2018](https://onlinelibrary.wiley.com/doi/full/10.1111/ddi.12778).
<br>
![](Predictors.PNG "List of Predictors")


Import data on primary forests
```{r, message=F, warning=F, cache=T}
pgdb <- "../_data/EPFDv2.0_DatabaseOA/EPFD_v2.0.mdb"
fc_list0 <- ogrListLayers(pgdb)
topen <- which(fc_list0=="EU_PrimaryForests_Polygons_OA_v20" )

primary.all <- readOGR(pgdb, fc_list0[topen]) 
primary.sf <- primary.all %>% 
  st_as_sf() %>% 
  mutate(is.primary=1) %>% 
  dplyr::select(is.primary)
```

Import AOI (area of interest) - Polygon of the Carpathians Mountain Range. Convert it to WGS84 to ensure compatibility with the data in the EPFD v2.0.
```{r, warning=F}
carpathians.sf <- read_sf("../_data/EuropeanMountainAreas/m_massifs_v1.shp") %>% 
  filter(name_mm == "Carpathians") %>% 
  st_transform(crs(primary.sf))

```


Select only primary forests inside the Carpathians and visualize
```{r, message=F, warning=F, fig.align="center", fig.height=5, fig.width=6}
contained <- st_contains(carpathians.sf, primary.sf)
primary.carp <- primary.sf[unique(unlist(contained)),]


carpathians.sf <- carpathians.sf %>% 
  st_set_crs(crs(primary.carp))

#visualize
ggplot()  + 
  geom_sf(data=carpathians.sf, col=NA, fill=4, alpha=0.5) + 
  geom_sf(data=primary.carp) + 
  theme_bw()
```

Rasterize `primary.carp` and add to stack of predictors
```{r, message=F, warning=F}
primary.raster <- rasterize(primary.carp, mystack)
mystack <- stack(primary.raster, mystack)
names(mystack)[1] <- "Y.Primary.forest"
```


## Model distribution of primary forests

To train our BRT, we need to export the data from our raster stack into a data.frame. We also need to mask out all pixels which are not forests.
```{r}
mydata0 <- getValues(mystack) %>% 
  as_tibble() 
mydata.coord <- raster::coordinates(mystack) %>% 
  as_tibble()
mydata <- mydata.coord %>% 
  bind_cols(mydata0) %>% 
  mutate(Y.Primary.forest=ifelse(!is.na(Y.Primary.forest), 1, 0)) %>% 
  ### mask only forested pixels. Exclude pixels with NA in GS
  filter(!is.na(X0.0b.mask) & !is.na(X3.2.Growing.Stock)) %>% 
  ### define biogeoregions as a factors
  mutate(X0.4.BiogeoRegions2016 = factor(X0.4.BiogeoRegions2016))

#visualize
glimpse(mydata)
```




### Naïve model
We fit a preliminary model. 
We call it naïve because we are not accounting for the extremely clutered distribution of our primary forest pixel, which might give rise to issues of autocorrelation.
Take ten times as many background points as presence points. 
Split in k=5 folds, four to be used as training, one as test dataset.
```{r}
mysample.naive <- mydata %>% 
  #count num of PF pixels
  mutate(sample.size=nrow(mydata %>% filter(Y.Primary.forest==1))) %>%  
  #set sample size for pixels to be randomly drawn. 
  #We keep all primary forests, and sample 10 times as many background points
  mutate(sample.size=ifelse(Y.Primary.forest==1, sample.size, 10*sample.size)) %>% 
  group_by(Y.Primary.forest) %>%
  sample_n(sample.size) %>% 
  ungroup() %>% 
  # split in k=5 folds
  mutate(kfold=kfold(Y.Primary.forest, k=5)) %>% 
  mutate(Y.Primary.forest=as.logical(Y.Primary.forest)) %>% 
  as.data.frame()

```

We will fit the model only on 80% of the data selected. The remaining 20% we keep for testing the goodness of our model

```{r fig.align="center", fig.height=8, fig.width=8}
train.data <- mysample.naive %>% 
  filter(kfold != 5)
test.data <- mysample.naive %>% 
  filter(kfold == 5)

#### run BRTs and explore output
brt.naive <- gbm.step(data=train.data, 
                      gbm.x = 5:19, gbm.y = "Y.Primary.forest",  
                      family = "bernoulli", tree.complexity = 5, 
                      learning.rate = 0.05, bag.fraction = 0.5)


summary.gbm(brt.naive, plotit = F)
gbm.plot(brt.naive, n.plots=6, plot.layout=c(3, 2), write.title = FALSE)
```
These graphs are called partial dependency plots, and show how the response variables varies as a function of one predictor at the time, when the other predictors are held constant at their respective means.

```{r fig.align="center", fig.height=8, fig.width=8}
# evaluate our model based on the test dataset.
(e <- dismo::evaluate(p = test.data %>% 
                filter(Y.Primary.forest==1), 
              a = test.data %>% 
                filter(Y.Primary.forest==0), 
              brt.naive, 
              n.trees=brt.naive$gbm.call$best.trees))

```

We have a very high Area Under the Curve, and a moderately high correlation.  
Compare these values with those of the brt self-statistics. Why are they lower?
<br>
Never heard of AUC, TPR and TNR? check [here](https://en.wikipedia.org/wiki/Receiver_operating_characteristic])


```{r}
## predict to the whole landscape
myp <- gbm::predict.gbm(brt.naive, 
                           getValues(mystack[[-1]]) %>% 
                             as.data.frame(), 
                           type="response")

myp.df <- raster::coordinates(mystack) %>% 
  as_tibble() %>% 
  bind_cols(brt.pred=myp) #%>% 
  #bind_cols(getValues(mystack[[c("X0.0b.mask.figs.UKR", "X3.2.Growing.Stock.UKR")]]) %>% 
  #            as_tibble()) %>% 
  #mutate(brt.pred=ifelse( (!is.na(X0.0b.mask.figs.UKR) & !is.na(X3.2.Growing.Stock.UKR)), brt.pred, NA))


myp.raster.naive <- raster(mystack[[1]])
myp.raster.naive <- setValues(myp.raster.naive, myp.df$brt.pred)
myp.raster.naive <- mask(myp.raster.naive, mask = mystack$X0.0b.mask)

plot(myp.raster.naive)
```


Build boxplots comparing the predicted values of primary vs. non-primary forests.
```{r, fig.height=4, fig.width=5}
mysample.naive$predicted <- gbm::predict.gbm(brt.naive, mysample.naive, type="response")
boxplot(predicted ~ Y.Primary.forest, data=mysample.naive)

```

The results look good. 
Too good.
This is because the model is severely overfitted. 
The problem mostly derives from the fact that presence data (primary forests) have a geographical distribution completely different from that of the background points.
Being all environmental data autocorrelated (at least to a certain extent), the clustering in presence data means that even those presence points not used for training the model are on average very similar to the points used for training the model.
This means that our cross-validation is overly optimistic because we are not respecting the assumption that train and test data are completely independent.
A more honest evaluation of our model could be obtained dividing train and test data in blocks based on their geographical distribution. 


### Spatially-aware model

To define the size of a spatial block, we first need to check for the spatial autocorrelation of predictors. We can do this using the package `blockCV`.
```{r, cache=T, warning=F}
mysample.naive.sf <- mysample.naive
coordinates(mysample.naive.sf) <- ~x+y
crs(mysample.naive.sf) <- crs(mystack)
mysample.naive.sf <- mysample.naive.sf %>% 
  st_as_sf() 


## Calculate spatial aucotorrelation of continuous predictors
sac <- spatialAutoRange(rasterLayer = mystack[[-c(1,5,14, 16)]],
                        sampleNumber = 5000,
                        doParallel = F,
                        showPlots = TRUE)
autocorrelation.i <- sac$range
print(paste("Autocorrelation is", round(autocorrelation.i/1000), "km"))
```

We can now divide our study regions in blocks. The function `spatialBlock` will help us assign each block to one of our k = 5 folds, in a way that the proportion of presence and background points remains balances.
```{r, cache=T, warning=F}
#spatial blocking with randomly assigned grid cells, 
sb2 <- spatialBlock(speciesData = mysample.naive.sf, 
                    species = "Y.Primary.forest",
                    rasterLayer = mystack[[2]],
                    theRange=autocorrelation.i, 
                    k = 5,
                    iteration=49,
                    selection = "random", 
                    seed=100,
                    progress = T)
```
After splitting our data in train and test dataset, we can refit the model and obtain a more accurate validation. 

```{r}
train.data.aware <- mysample.naive %>% 
  filter(sb2$foldID != 5) 
test.data.aware <- mysample.naive %>% 
  filter(sb2$foldID != 5) 


#### run BRTs and explore output
brt.aware <- gbm.step(data=train.data.aware,
                        gbm.x = 5:19, gbm.y = "Y.Primary.forest",  
                        family = "bernoulli", tree.complexity = 5, 
                        learning.rate = 0.05, bag.fraction = 0.5)

(e <- evaluate(test.data.aware %>% 
                filter(Y.Primary.forest==1), 
              test.data.aware %>% 
                filter(Y.Primary.forest==0), 
              brt.aware, 
              n.trees=brt.aware$gbm.call$best.trees))
```
Notice how AUC and correlation changed, compared to the model test.statistics of the our naive model.
<br>

Congralations!  
You have scratched the surface of a powerful technique for creating spatial distribution models using a machine-learning algorithm. 






